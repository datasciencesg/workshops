# Overview

- **Chapter 3: Recurrent Neural Nets, Self-Attention Nets and Natural Language Processing**

  - 3.1 *Introduction to Natural Language Processing*
 
  - 3.2 *Recurrent Neural Nets*
    - Blogposts that presents RNN
    - Show me the Code
    - Show me the Math
 
  - 3.3 *Self-Attention Nets* (aka Transformer)
     - Blogposts that presents Transformer
     - Show me the Code
     - Show me the Math
   
 - 3.4 *Natural Language Processing (for real)*
   
----


### 3.1: **Introduction to Natural Language Processing**


 - [Lisbon Machine Learning School (LxMLS 2018)](http://lxmls.it.pt/2018/?page_id=19) (focuses a lot on NLP in 2018)
 - Saarland University [Foundations in Language Sciencea and Technology (FLST) course](http://www.coli.uni-saarland.de/courses/FLST/2018/FLST.html) 
 
 - [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/) (Jurafsky and Martin's Book) 
 - [Foundations of Statistical Natural Language Processing](https://nlp.stanford.edu/fsnlp/) (Chris Manning's book, hardcore math)
 - [Computational Linguistics and Deep Learning](https://www.mitpressjournals.org/doi/pdf/10.1162/COLI_a_00239) (Chris Manning's words in 2015)
 - [A Primer on Neural Network Models for Natural Language Processing](https://u.cs.biu.ac.il/~yogo/nnlp.pdf) (Yoav Goldberg's book/chapter)
   - physical book version on https://www.morganclaypool.com/doi/abs/10.2200/S00762ED1V01Y201703HLT037

   
**Related but not directly**

  - [Artificial Intelligence: A Modern Approach](http://aima.cs.berkeley.edu/) (Russell and Norvig's book)
    - Borrow from national library, click [here](https://catalogue.nlb.gov.sg/cgi-bin/spydus.exe/FULL/WPAC/BIBENQ/13461273/269039522,1) =) 
  - [Deep Learning](https://www.deeplearningbook.org/) (Goodfellow et al. book)
  
   
  
----


### 3.2: **Recurrent Neural Nets**


  Show me the code
    - https://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/


----


### 3.3: **Self-Attention Nets** (aka Transformer)

  - Blogpost that explains Self-Attention Nets
    - http://jalammar.github.io/illustrated-transformer/
    - https://ai.googleblog.com/2017/08/transformer-novel-neural-network.htm
    - https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attentionis-all-you-need/

  - Show me the code
    - http://nlp.seas.harvard.edu/2018/04/03/attention.html
    - https://medium.com/@kolloldas/building-the-mighty-transformer-for-sequence-tagging-in-pytorch-part-i-a1815655cd8 

----


### 3.4: Natural Language Processing (*for real*) 

Natural Language Processing evolves really fast and as a field that mostly adopts/applies the "hottest" trend from mainstream machine/deep learning, every ~2-3 years the underlying state-of-art architecture for NLP changes (quite drastically). 

The only way to keep up is not to chase the trends. Know that the lastest/hottest algorithm or pre-trained models exists, understand how they work and find ways to incorporate them into whichever project/method you're using/researching on. 

There's really no point in chasing any method (in NLP or any other sub-fields of computing) because 

 - if you're in the industry, what usually happens is that you try every possible method until you find  that fits your needs
 - if you're in academia, chasing state-of-art is going to drain you out and make your work mediorce/common, focus on the story and why things work more than how to make model X work on task Y.
   
In 2018, here's some recent trends: https://arxiv.org/pdf/1708.02709.pdf

